# -*- coding: utf-8 -*-
"""Selekcja zmiennych i PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OvTogMuMcDVm1MoM_q69l7A7dXb9RWLG
"""

from google.colab import files
uploaded = files.upload()
#Korzystam ze zbiorów titanic.csv i pima.csv

import numpy as np
import pandas
from matplotlib import pyplot as plt 
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel,SequentialFeatureSelector

#Zbiór danych titanic.csv zawiera dane osób, które płynęły Titaniciem oraz informację czy przeżyły
#Dopasowując model uczenia maszynowego określę, które parametry są w nim istotne

#Zad 1
#Wczytuję dane i dzielę je losowo na zbiór uczący i testowy
url = 'titanic.csv'
dataframe = pandas.read_csv(url,header=0)
data = dataframe.values
X, y = data[:,1:], data[:, 0]
X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,test_size = 0.3)

#Dopasowuję model regresji logistycznej i sprawdzam dokładność
m = LogisticRegression(penalty='none')
m.fit(X_train,y_train)
print(m.score(X_test,y_test))
m.coef_

#Selektor wybiera zmienne znaczące dla zadanego estymatora
selector = SelectFromModel(estimator = LogisticRegression(penalty='none'))
selector.fit(X_train,y_train)
#print(selector.estimator_.coef_)
#print(selector.threshold_)
#Wyświetlam listę znaczących parametrów
print(selector.get_support())
X_train2 = selector.transform(X_train)
X_test2 = selector.transform(X_test)

m.fit(X_train2,y_train)
print(m.score(X_test2,y_test))

#Dokładność nowego (mniejszego) modelu jest niższa, ale udało nam się znacząco zmniejszyć liczbę parametrów

#Działa tak samo, ale wybiera ile chcemy, więc możemy wziąć trzeci parametr
sfs = SequentialFeatureSelector(LogisticRegression(penalty='none'),n_features_to_select=3)
sfs.fit(X_train,y_train)
print(sfs.get_support())
X_train3 = sfs.transform(X_train)
X_test3 = sfs.transform(X_test)
m.fit(X_train3,y_train)
print(m.score(X_test3,y_test))

from google.colab import files
uploaded = files.upload()

#Zad 2
url = 'pima.csv'
dataframe = pandas.read_csv(url,header=0)
data = dataframe.values
X, y = data[:,:-1], data[:, -1]
X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,test_size = 0.3)
print(X_train.shape)

#Wyznaczamy komponenty metodą PCA
pca = PCA()
scaler=StandardScaler()
X_train2 = scaler.fit_transform(X_train)
X_test2 = scaler.transform(X_test)
pca.fit(X_train2)
print(pca.components_)
#Na wykresie widać jaki procent wariancji jest wyjaśniany przez poszczególne zmienne
plt.plot(pca.explained_variance_ratio_)

#Transformowanie przy pomocy wektorów własnych
X_train_2 = pca.transform(X_train2)
X_test_2 = pca.transform(X_test2)