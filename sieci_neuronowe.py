# -*- coding: utf-8 -*-
"""Sieci neuronowe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8gk2kpi01oOxLHzThOV8U2NbxznSqGS
"""

from google.colab import files
uploaded = files.upload()
#Używałem zbioru bank-full-encoded.csv

from keras.datasets import mnist
import numpy as np
import pandas as pd
from matplotlib import pyplot
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline

#Celem pierwszego zadania jest porównanie dokładności sieci neuronowych z modelem logistycznej regresji

#Zad 1
#Wczytywanie danych i losowy podział na zbiory uczące i testowe
url = 'bank-full-encoded (1).csv'
dataframe = pd.read_csv(url,header=0)
data = dataframe.values
X, y = data[:,:-1], data[:, -1]
X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,test_size = 0.3)

#Na początku porównam sieć neuronową z modelem logistycznej regresji
#Będę musiał przeskalować zmienne objaśniające
scal = StandardScaler()
#Dopasowuję dwa modele, których dokładność będę porównywał
#1. Sieć neuronowa o zerowej liczbie warstw ukrytych z logistyczną funkcją aktywacji
m1 = MLPClassifier(hidden_layer_sizes=(), solver='lbfgs',alpha=0,max_iter=1000,activation='logistic')
#2. Logistyczna regresja
m2 = LogisticRegression(penalty = 'none',max_iter = 1000)
#Używam pipe'a żeby skalować dane i używać odpowiednich modeli
pipe1 = Pipeline(steps = [("scal",scal),('m',m1)])
pipe2 = Pipeline(steps = [("scal",scal),('m',m2)])
pipe1.fit(X_train,y_train)
pipe2.fit(X_train,y_train)
#Na końcu wypisuję otrzymane współczynniki
classifier1 = pipe1.named_steps['m']
print(classifier1.coefs_)
classifier2 = pipe2.named_steps['m']
print(classifier2.coef_)

#Teraz rozważę modele, które mają 1, 2 lub 3 warstywy ukryte
p = X_train.shape[1] #liczba kolumn (zmiennych objaśniających)
p2 = int(p/2)
#Tworzę siatkę parametrów przy założeniu, że każda warstwa może mieć rozmiar p lub p/2
param_grid= {"m__hidden_layer_sizes":[(),(p2),(p),
                                      (p2,p2),(p2,p),(p,p2),(p,p),
                                      (p2,p2,p2),(p2,p2,p),(p2,p,p2),(p,p2,p2),
                                      (p,p,p2),(p,p2,p),(p2,p,p),(p,p,p)]}
#Wybieramy najlepszy model
search = GridSearchCV(pipe1,param_grid,n_jobs=-1)
search.fit(X_train,y_train)
print(search.best_params_)

#Porównanie współczynnika AUC najlepszej sieci neuronowej z modelem regresji logistycznej
search_probs = search.predict_proba(X_test)[:, 1]
print(roc_auc_score(y_test, search_probs))
log_probs = pipe2.predict_proba(X_test)[:, 1]
print(roc_auc_score(y_test, log_probs))

#W kolejnym zadaniu dopasuję sieć neuronową czytającą liczby z rysnuków i obliczę ich dokładności dla różnych funkcji aktywacji

#Zad 2
#Korzystam ze zbioru danych minst
(train_X, train_y), (test_X, test_y) = mnist.load_data()
#Każda obserwacja to macierz z wartościami od 1 do 255
print(test_X.shape)
print(train_X[0])

#Każda obserwacja to rysunek jakiejś liczby, tak się je wizualizuje:
for i in range(9):  
  pyplot.subplot(331)
  pyplot.imshow(train_X[i], cmap=pyplot.get_cmap('gray'))
  pyplot.show()

#zmieniamy wartości na (0,1)
train_X = train_X/255
test_X = test_X/255
train_X = train_X.reshape(60000,784) #zmieniamy na wektor
test_X = test_X.reshape(10000,784)

#Dopasowuję do danych sieci neuronowe z funkcjami aktywacji: relu i logistyczną
mlp1= MLPClassifier(hidden_layer_sizes=(100),alpha = 0,max_iter=100) #domyślnie jest relu
mlp1.fit(train_X,train_y)
mlp2= MLPClassifier(hidden_layer_sizes=(100),activation='logistic',alpha = 0,max_iter=100)
mlp2.fit(train_X,train_y)
#Wyświetlam ich dokładności
print(mlp1.score(test_X,test_y))
print(mlp2.score(test_X,test_y))